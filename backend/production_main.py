#!/usr/bin/env python3
"""
Production-Ready Okapiq Backend
Real API integrations, proper error handling, client-ready
"""

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any
import asyncio
import logging
import time
from datetime import datetime
import uvicorn
import json
from typing import Optional
from math import isfinite

# Configure production logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import core components
try:
    from app.core.config import settings
    from app.core.database import get_db, engine, Base
    logger.info("‚úÖ Core imports successful")
except Exception as e:
    logger.error(f"‚ùå Core import failed: {e}")
    raise

# Import services with fallbacks
try:
    from app.services.enhanced_market_intelligence_service import EnhancedMarketIntelligenceService
    from app.crawlers.smart_crawler_hub import SmartCrawlerHub, CrawlerType
    from app.processors.data_normalizer import DataNormalizer
    from app.enrichment.enrichment_engine import EnrichmentEngine
    logger.info("‚úÖ Intelligence services imported")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Intelligence services import failed: {e}")
    # Create fallback classes
    class EnhancedMarketIntelligenceService:
        async def get_comprehensive_market_data(self, location, industry=None, radius_miles=25):
            return {"businesses": [], "market_metrics": {}, "data_sources": {}}
    
    class SmartCrawlerHub:
        async def crawl_business_data(self, location, industry=None, sources=None):
            return {}
    
    class DataNormalizer:
        def normalize_crawl_results(self, crawl_results, merge_duplicates=True):
            return []
    
    class EnrichmentEngine:
        async def enrich_businesses(self, businesses, enrichment_types=None):
            return businesses

# Create database tables
try:
    Base.metadata.create_all(bind=engine)
    logger.info("‚úÖ Database tables created")
except Exception as e:
    logger.error(f"‚ùå Database setup failed: {e}")

app = FastAPI(
    title="Okapiq Intelligence Platform",
    description="Bloomberg for Small Businesses - Production-ready market intelligence with real API integrations",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS.split(",") + ["https://app.okapiq.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize services
market_service = EnhancedMarketIntelligenceService()
crawler_hub = SmartCrawlerHub()
data_normalizer = DataNormalizer()
enrichment_engine = EnrichmentEngine()

class MarketScanRequest(BaseModel):
    location: str
    industry: Optional[str] = None
    radius_miles: Optional[int] = 25

class Business(BaseModel):
    name: str
    address: Optional[str] = ""
    phone: Optional[str] = ""
    website: Optional[str] = ""
    rating: Optional[float] = 0.0
    review_count: Optional[int] = 0
    estimated_revenue: Optional[int] = 0
    employee_count: Optional[int] = 0
    years_in_business: Optional[int] = 0
    succession_risk_score: Optional[int] = 0
    owner_age_estimate: Optional[int] = 0
    market_share_percent: Optional[float] = 0.0
    lead_score: Optional[int] = 0
    opportunity_score: Optional[int] = 0
    badges: Optional[List[str]] = []
    data_sources: Optional[List[str]] = []
    coordinates: Optional[List[float]] = None
    selling_signals: Optional[Dict[str, Any]] = None
    enriched_data: Optional[Dict[str, Any]] = None

class MarketScanResponse(BaseModel):
    location: str
    industry: str
    business_count: int
    businesses: List[Business]
    market_intelligence: Dict[str, Any]
    data_sources: List[str]
    scan_metadata: Dict[str, Any]
    tam_estimate: Optional[int] = 0
    sam_estimate: Optional[int] = 0
    som_estimate: Optional[int] = 0
    hhi_score: Optional[float] = 0
    fragmentation_level: Optional[str] = "unknown"
    avg_revenue_per_business: Optional[int] = 0
    market_saturation_percent: Optional[float] = 0
    ad_spend_to_dominate: Optional[int] = 0
    berkeley_integration: Optional[Dict[str, Any]] = {}

@app.get("/")
async def root():
    """Root endpoint with platform information"""
    return {
        "message": "üöÄ Okapiq Intelligence Platform",
        "version": "2.0.0",
        "description": "Bloomberg for Small Businesses - Production Ready",
        "status": "operational",
        "features": [
            "Real API integrations (Census, SerpAPI, Data Axle)",
            "Fuzzy business deduplication",
            "AI-powered opportunity scoring",
            "Dynamic badge system",
            "Market intelligence pipeline",
            "Production error handling",
            "Client-ready interface"
        ],
        "endpoints": {
            "market_scan": "/market/scan",
            "docs": "/docs",
            "health": "/health"
        },
        "api_keys_configured": {
            "census": bool(settings.CENSUS_API_KEY),
            "serpapi": bool(settings.SERPAPI_API_KEY),
            "data_axle": bool(settings.DATA_AXLE_API_KEY),
            "yelp": bool(settings.YELP_API_KEY),
            "google_maps": bool(settings.GOOGLE_MAPS_API_KEY)
        }
    }

@app.post("/market/scan", response_model=MarketScanResponse)
async def production_market_scan(
    request: MarketScanRequest, 
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    PRODUCTION market scan with real API integrations
    Uses your Census, SerpAPI, Data Axle keys for live data
    """
    start_time = time.time()
    scan_id = f"scan_{int(time.time())}"
    
    logger.info(f"üöÄ Starting production market scan {scan_id}: {request.location}, {request.industry}")
    
    try:
        # STEP 1: Real API data collection
        logger.info("üì° Collecting data from real APIs...")
        
        # Use all available crawler sources
        crawler_sources = [
            CrawlerType.GOOGLE_MAPS,
            CrawlerType.YELP,
            CrawlerType.SERPAPI,
            CrawlerType.DATAAXLE,
            CrawlerType.SBA_RECORDS,
            CrawlerType.BIZBUYSELL
        ]
        
        # Crawl with real APIs
        crawl_results = await crawler_hub.crawl_business_data(
            location=request.location,
            industry=request.industry,
            sources=crawler_sources
        )
        
        logger.info(f"üìä Crawl results: {len(crawl_results)} sources")
        
        # STEP 2: Data normalization and deduplication
        logger.info("üîÑ Normalizing and deduplicating businesses...")
        
        normalized_businesses = data_normalizer.normalize_crawl_results(
            crawl_results,
            merge_duplicates=True
        )
        
        logger.info(f"üìã Normalized: {len(normalized_businesses)} unique businesses")
        
        # STEP 3: Real enrichment with your APIs
        logger.info("üåü Enriching with Census, IRS, SOS data...")
        
        enriched_businesses = await enrichment_engine.enrich_businesses(
            normalized_businesses,
            enrichment_types=['census', 'irs', 'sos', 'nlp', 'market_intelligence']
        )
        
        logger.info(f"‚ú® Enriched: {len(enriched_businesses)} businesses")
        
        # STEP 4: Get comprehensive market intelligence
        logger.info("üß† Computing market intelligence...")
        
        comprehensive_data = await market_service.get_comprehensive_market_data(
            location=request.location,
            industry=request.industry,
            radius_miles=request.radius_miles
        )
        
        # STEP 5: Convert to response format with opportunity scoring
        businesses_response = []
        for business_data in comprehensive_data.get('businesses', []):
            business = Business(
                name=business_data.get('name', 'Unknown Business'),
                address=business_data.get('address', ''),
                phone=business_data.get('phone', ''),
                website=business_data.get('website', ''),
                rating=business_data.get('rating', 0.0),
                review_count=business_data.get('review_count', 0),
                estimated_revenue=business_data.get('estimated_revenue', 0),
                employee_count=business_data.get('employee_count', 0),
                years_in_business=business_data.get('years_in_business', 0),
                succession_risk_score=business_data.get('succession_risk_score', 0),
                owner_age_estimate=business_data.get('owner_age_estimate', 0),
                market_share_percent=business_data.get('market_share_percent', 0.0),
                lead_score=business_data.get('lead_score', 0),
                opportunity_score=business_data.get('opportunity_score', 0),
                badges=business_data.get('badges', []),
                data_sources=business_data.get('data_sources', []),
                coordinates=business_data.get('coordinates'),
                selling_signals=business_data.get('selling_signals'),
                enriched_data=business_data.get('enriched_data', {})
            )
            businesses_response.append(business)
        
        # Sort by opportunity score
        businesses_response.sort(key=lambda b: b.opportunity_score or 0, reverse=True)
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Extract market metrics
        market_metrics = comprehensive_data.get('market_metrics', {})
        
        # Build response
        response = MarketScanResponse(
            location=request.location,
            industry=request.industry or "general",
            business_count=len(businesses_response),
            businesses=businesses_response,
            market_intelligence={
                **market_metrics,
                "processing_time": f"{processing_time:.2f}s",
                "scan_id": scan_id,
                "high_opportunity_count": len([b for b in businesses_response if (b.opportunity_score or 0) > 80]),
                "succession_targets": len([b for b in businesses_response if "Succession Target" in (b.badges or [])]),
                "market_leaders": len([b for b in businesses_response if "Market Leader" in (b.badges or [])]),
                "avg_opportunity_score": sum(b.opportunity_score or 0 for b in businesses_response) / max(len(businesses_response), 1)
            },
            data_sources=list(comprehensive_data.get('data_sources', {}).keys()),
            scan_metadata={
                "timestamp": datetime.now().isoformat(),
                "scan_id": scan_id,
                "processing_time": f"{processing_time:.2f}s",
                "api_keys_used": [
                    key for key, configured in {
                        "census": bool(settings.CENSUS_API_KEY),
                        "serpapi": bool(settings.SERPAPI_API_KEY),
                        "data_axle": bool(settings.DATA_AXLE_API_KEY),
                        "yelp": bool(settings.YELP_API_KEY),
                        "google_maps": bool(settings.GOOGLE_MAPS_API_KEY)
                    }.items() if configured
                ],
                "pipeline_stages": ["crawling", "normalization", "enrichment", "scoring", "ranking"],
                "businesses_found": len(businesses_response),
                "enrichment_success_rate": f"{len(enriched_businesses)}/{len(normalized_businesses) or 1}"
            },
            tam_estimate=market_metrics.get('tam_estimate', 0),
            sam_estimate=market_metrics.get('sam_estimate', 0),
            som_estimate=market_metrics.get('som_estimate', 0),
            hhi_score=market_metrics.get('hhi_score', 0),
            fragmentation_level=market_metrics.get('fragmentation_level', 'unknown'),
            avg_revenue_per_business=market_metrics.get('avg_revenue_per_business', 0),
            market_saturation_percent=market_metrics.get('market_saturation_percent', 0),
            ad_spend_to_dominate=market_metrics.get('ad_spend_to_dominate', 0),
            berkeley_integration=comprehensive_data.get('berkeley_research', {})
        )
        
        logger.info(f"‚úÖ Market scan {scan_id} completed in {processing_time:.2f}s: {len(businesses_response)} businesses")
        
        return response
        
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"‚ùå Market scan {scan_id} failed after {processing_time:.2f}s: {str(e)}")
        
        # Return error response with context
        raise HTTPException(
            status_code=500, 
            detail={
                "error": "Market scan failed",
                "message": str(e),
                "scan_id": scan_id,
                "processing_time": f"{processing_time:.2f}s",
                "location": request.location,
                "industry": request.industry,
                "api_keys_configured": {
                    "census": bool(settings.CENSUS_API_KEY),
                    "serpapi": bool(settings.SERPAPI_API_KEY),
                    "data_axle": bool(settings.DATA_AXLE_API_KEY)
                }
            }
        )

@app.get("/market/scan")
async def market_scan_get(
    location: str = "San Francisco", 
    industry: str = "HVAC", 
    radius_miles: int = 25,
    db: Session = Depends(get_db)
):
    """GET version for browser testing"""
    request = MarketScanRequest(location=location, industry=industry, radius_miles=radius_miles)
    background_tasks = BackgroundTasks()
    return await production_market_scan(request, background_tasks, db)

@app.get("/health")
async def health_check():
    """Production health check with API status"""
    api_status = {}
    
    # Check API key configuration
    api_keys = {
        "census": settings.CENSUS_API_KEY,
        "serpapi": settings.SERPAPI_API_KEY,
        "data_axle": settings.DATA_AXLE_API_KEY,
        "yelp": settings.YELP_API_KEY,
        "google_maps": settings.GOOGLE_MAPS_API_KEY
    }
    
    for api_name, api_key in api_keys.items():
        api_status[api_name] = {
            "configured": bool(api_key),
            "status": "ready" if api_key else "missing_key"
        }
    
    return {
        "status": "healthy",
        "platform": "okapiq_production",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "api_integrations": api_status,
        "features": [
            "real_api_data",
            "fuzzy_deduplication",
            "opportunity_scoring",
            "census_enrichment",
            "market_intelligence",
            "production_logging"
        ],
        "database": "connected",
        "ready_for_clients": True
    }

@app.get("/api-status")
async def api_status():
    """Detailed API integration status for debugging"""
    status = {}
    
    # Test each API configuration
    apis = {
        "Census API": {
            "key_configured": bool(settings.CENSUS_API_KEY),
            "base_url": settings.CENSUS_BASE_URL,
            "description": "US demographic and economic data"
        },
        "SerpAPI": {
            "key_configured": bool(settings.SERPAPI_API_KEY),
            "base_url": settings.SERPAPI_BASE_URL,
            "description": "Google Maps business search results"
        },
        "Data Axle": {
            "key_configured": bool(settings.DATA_AXLE_API_KEY),
            "base_url": settings.DATAAXLE_BASE_URL,
            "description": "Business directory and contact data"
        },
        "Yelp API": {
            "key_configured": bool(settings.YELP_API_KEY),
            "base_url": settings.YELP_BASE_URL,
            "description": "Business reviews and ratings"
        },
        "Google Maps": {
            "key_configured": bool(settings.GOOGLE_MAPS_API_KEY),
            "base_url": "https://maps.googleapis.com/maps/api",
            "description": "Location and places data"
        }
    }
    
    total_configured = sum(1 for api in apis.values() if api["key_configured"])
    
    return {
        "total_apis": len(apis),
        "configured_apis": total_configured,
        "configuration_percentage": f"{(total_configured/len(apis)*100):.1f}%",
        "apis": apis,
        "recommendations": [
            "Set missing API keys in environment variables",
            "Test individual API endpoints",
            "Monitor API usage and rate limits",
            "Configure fallback data sources"
        ] if total_configured < len(apis) else [
            "All APIs configured successfully!",
            "Ready for production usage",
            "Monitor API quotas and usage"
        ]
    }

# Include existing routers if available (isolate failures per router)
try:
    from app.routers import auth
    app.include_router(auth.router, prefix="/auth", tags=["Authentication"])
    logger.info("‚úÖ Auth router included")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Auth router not available: {e}")

try:
    from app.routers import leads
    app.include_router(leads.router, prefix="/leads", tags=["Lead Management"])
    logger.info("‚úÖ Leads router included")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Leads router not available: {e}")

try:
    from app.routers import analytics
    app.include_router(analytics.router, prefix="/analytics", tags=["Analytics"])
    logger.info("‚úÖ Analytics router included")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Analytics router not available: {e}")

try:
    from app.routers import market
    app.include_router(market.router, prefix="/market", tags=["Market Analysis"])
    logger.info("‚úÖ Market router included")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Market router not available: {e}")

try:
    from app.routers import intelligence
    app.include_router(intelligence.router, prefix="/intelligence", tags=["Intelligence"])
    logger.info("‚úÖ Intelligence router included")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Intelligence router not available: {e}")

# Lightweight proxy-check endpoint (no dependency on intelligence router)
@app.get("/intelligence/proxy-check")
async def proxy_check(target: str = "https://api.ipify.org?format=json"):
    """Check outbound IP using configured proxies (samples from rotation)."""
    try:
        from app.core.proxy_manager import ProxyManager
        from app.core.config import settings as cfg
        import aiohttp

        pm = ProxyManager(cfg.PROXIES or "", cfg.PROXY_USERNAME, cfg.PROXY_PASSWORD)
        if not pm.has_proxies():
            return {"using_proxies": False, "message": "No proxies configured"}

        samples = []
        total = min(3, len((cfg.PROXIES or '').split(','))) or 1
        for _ in range(total):
            kwargs = pm.aiohttp_kwargs()
            async with aiohttp.ClientSession() as session:
                async with session.get(target, timeout=10, **kwargs) as resp:
                    ct = resp.headers.get("Content-Type", "")
                    if "json" in ct:
                        data = await resp.json()
                    else:
                        txt = await resp.text()
                        try:
                            data = json.loads(txt)
                        except Exception:
                            data = {"raw": txt[:120]}
                    samples.append(data)
        return {"using_proxies": True, "samples": samples}
    except Exception as e:
        return {"using_proxies": False, "error": str(e)}

# Live data endpoints (bypass services; return full payloads)
@app.get("/market/raw/google-serpapi")
async def raw_google_serpapi(location: str, industry: Optional[str] = None):
    try:
        from app.data_collectors.serpapi_client import SerpAPIClient
        client = SerpAPIClient()
        data = await client.search_raw(location=location, industry=industry)
        return data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"SerpAPI raw fetch failed: {str(e)}")

@app.get("/market/raw/dataaxle")
async def raw_dataaxle(location: str, industry: Optional[str] = None, limit: int = 50):
    try:
        from app.data_collectors.dataaxle_api import DataAxleAPI
        client = DataAxleAPI()
        businesses = await client.search_businesses(location=location, industry=industry, limit=limit)
        return {"businesses": businesses}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Data Axle fetch failed: {str(e)}")

@app.get("/market/live-scan")
async def market_live_scan(location: str, industry: Optional[str] = None, limit: int = 50):
    """Aggregate SerpAPI and Data Axle live data and return both raw and normalized."""
    try:
        from app.data_collectors.serpapi_client import SerpAPIClient
        from app.data_collectors.dataaxle_api import DataAxleAPI
        from app.data_collectors.google_places_client import GooglePlacesClient
        from app.data_collectors.yelp_api_client import YelpAPIClient
        serp = SerpAPIClient()
        axle = DataAxleAPI()
        gmaps = GooglePlacesClient()
        yelp = YelpAPIClient()
        serp_raw = {}
        axle_list = []
        gmaps_list = []
        yelp_list = []
        # Fetch each source independently; do not fail whole if one fails
        try:
            serp_raw = await serp.search_raw(location=location, industry=industry)
        except Exception as e:
            serp_raw = {"error": str(e)}
        try:
            axle_list = await axle.search_businesses(location=location, industry=industry, limit=limit)
        except Exception as e:
            axle_list = [{"error": str(e)}]
        try:
            gmaps_list = await gmaps.search_businesses(location=location, industry=industry, pages=2)
        except Exception as e:
            gmaps_list = [{"error": str(e)}]
        try:
            yelp_list = await yelp.search(term=industry or "businesses", location=location, limit=min(50, limit))
        except Exception as e:
            yelp_list = [{"error": str(e)}]
        # Quick normalize
        businesses = []
        for r in serp_raw.get("local_results", []) or []:
            businesses.append({
                "name": r.get("title") or r.get("name"),
                "address": r.get("address"),
                "phone": r.get("phone"),
                "website": r.get("website"),
                "rating": r.get("rating"),
                "review_count": r.get("reviews"),
                "coordinates": r.get("gps_coordinates"),
                "source": "serpapi"
            })
        for a in axle_list or []:
            businesses.append({
                "name": a.get("business_name") or a.get("name"),
                "address": a.get("address"),
                "phone": a.get("phone"),
                "website": a.get("website"),
                "rating": a.get("rating"),
                "review_count": a.get("review_count"),
                "coordinates": a.get("coordinates"),
                "estimated_revenue": a.get("revenue") or a.get("estimated_revenue"),
                "employee_count": a.get("employee_count"),
                "source": "dataaxle"
            })
        for g in gmaps_list or []:
            businesses.append({
                "name": g.get("name"),
                "address": g.get("formatted_address"),
                "rating": g.get("rating"),
                "review_count": g.get("user_ratings_total"),
                "coordinates": g.get("geometry", {}).get("location"),
                "source": "google_places"
            })
        for y in yelp_list or []:
            businesses.append({
                "name": y.get("name"),
                "address": ", ".join(y.get("location",{}).get("display_address", [])),
                "phone": y.get("display_phone"),
                "website": y.get("url"),
                "rating": y.get("rating"),
                "review_count": y.get("review_count"),
                "coordinates": y.get("coordinates"),
                "source": "yelp"
            })
        return {
            "location": location,
            "industry": industry,
            "business_count": len(businesses),
            "businesses": businesses,
            "raw": {
                "serpapi": serp_raw,
                "dataaxle": {"businesses": axle_list},
                "google_places": {"results": gmaps_list},
                "yelp": {"businesses": yelp_list}
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Live scan failed: {str(e)}")

@app.get("/market/raw/google-places")
async def raw_google_places(location: str, industry: Optional[str] = None, pages: int = 2):
    try:
        from app.data_collectors.google_places_client import GooglePlacesClient
        g = GooglePlacesClient()
        res = await g.search_businesses(location=location, industry=industry, pages=pages)
        return {"results": res}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Google Places fetch failed: {str(e)}")

@app.get("/market/raw/yelp")
async def raw_yelp(location: str, industry: Optional[str] = None, limit: int = 20):
    try:
        from app.data_collectors.yelp_api_client import YelpAPIClient
        y = YelpAPIClient()
        res = await y.search(term=industry or "businesses", location=location, limit=limit)
        return {"businesses": res}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Yelp fetch failed: {str(e)}")

@app.get("/market/live-analyze")
async def market_live_analyze(
    location: str,
    query: Optional[str] = None,
    limit: int = 50,
    pages: int = 2,
    include_raw: bool = False
):
    """
    Aggregate Google Places + Yelp in real-time for ANY industry/service and return:
    - normalized businesses
    - market metrics (TAM proxy, HHI/fragmentation, ratings, review stats)
    - source counts
    """
    try:
        from app.data_collectors.google_places_client import GooglePlacesClient
        from app.data_collectors.yelp_api_client import YelpAPIClient
        gmaps = GooglePlacesClient()
        yelp = YelpAPIClient()

        g_list = []
        y_list = []
        try:
            g_list = await gmaps.search_businesses(location=location, industry=query, pages=pages)
        except Exception as e:
            g_list = [{"error": str(e)}]
        try:
            y_list = await yelp.search(term=query or "businesses", location=location, limit=min(50, limit))
        except Exception as e:
            y_list = [{"error": str(e)}]

        businesses = []
        for g in g_list or []:
            if "error" in g:
                continue
            businesses.append({
                "name": g.get("name"),
                "address": g.get("formatted_address"),
                "phone": None,
                "website": None,
                "rating": g.get("rating"),
                "review_count": g.get("user_ratings_total"),
                "coordinates": (g.get("geometry", {}) or {}).get("location"),
                "price_level": g.get("price_level"),
                "source": "google_places"
            })
        for y in y_list or []:
            if "error" in y:
                continue
            businesses.append({
                "name": y.get("name"),
                "address": ", ".join((y.get("location", {}) or {}).get("display_address", [])),
                "phone": y.get("display_phone"),
                "website": y.get("url"),
                "rating": y.get("rating"),
                "review_count": y.get("review_count"),
                "coordinates": y.get("coordinates"),
                "price_level": None,
                "source": "yelp"
            })

        seen = set()
        deduped = []
        for b in businesses:
            key = ((b.get("name") or "").strip().lower(), (b.get("address") or "").strip().lower())
            if key in seen:
                continue
            seen.add(key)
            deduped.append(b)

        valid_reviews = [b.get("review_count") for b in deduped if isinstance(b.get("review_count"), (int, float))]
        total_reviews = sum(valid_reviews) if valid_reviews else 0
        ratings = [b.get("rating") for b in deduped if isinstance(b.get("rating"), (int, float))]
        avg_rating = sum(ratings) / len(ratings) if ratings else 0.0

        hhi = 0.0
        if total_reviews > 0:
            for b in deduped:
                rv = b.get("review_count") or 0
                if isinstance(rv, (int, float)) and rv > 0:
                    share = rv / total_reviews
                    hhi += (share * share) * 10000
        fragmentation_level = "fragmented"
        if hhi > 2500:
            fragmentation_level = "highly_concentrated"
        elif hhi > 1500:
            fragmentation_level = "moderately_concentrated"

        tam_proxy = int(total_reviews * 500)
        top = sorted([b for b in deduped if isinstance(b.get("review_count"), (int, float))], key=lambda x: x.get("review_count", 0), reverse=True)[:10]

        result = {
            "location": location,
            "industry": query or "general",
            "business_count": len(deduped),
            "businesses": deduped,
            "market_intelligence": {
                "tam_estimate_proxy": tam_proxy,
                "avg_rating": round(avg_rating, 2) if isfinite(avg_rating) else 0,
                "total_reviews": total_reviews,
                "hhi_index": round(hhi, 2),
                "fragmentation_level": fragmentation_level,
                "top_by_reviews": [{"name": b.get("name"), "review_count": b.get("review_count"), "rating": b.get("rating"), "source": b.get("source")} for b in top]
            },
            "data_sources": {
                "google_places_count": len(g_list) if isinstance(g_list, list) else 0,
                "yelp_count": len(y_list) if isinstance(y_list, list) else 0
            }
        }
        if include_raw:
            result["raw_samples"] = {
                "google_places": (g_list[:5] if isinstance(g_list, list) else g_list),
                "yelp": (y_list[:5] if isinstance(y_list, list) else y_list)
            }
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Live analyze failed: {str(e)}")

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Global exception handler for production"""
    logger.error(f"Global exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": "An unexpected error occurred",
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )

if __name__ == "__main__":
    logger.info("üöÄ Starting Okapiq Production Server...")
    logger.info("üîë API Key Status:")
    
    apis = ["CENSUS_API_KEY", "SERPAPI_API_KEY", "DATA_AXLE_API_KEY", "YELP_API_KEY", "GOOGLE_MAPS_API_KEY"]
    for api in apis:
        status = "‚úÖ CONFIGURED" if getattr(settings, api) else "‚ùå MISSING"
        logger.info(f"   {api}: {status}")
    
    logger.info("üåê Server starting on http://localhost:8000")
    logger.info("üìñ API docs: http://localhost:8000/docs")
    logger.info("üîç API status: http://localhost:8000/api-status")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info",
        access_log=True
    )
